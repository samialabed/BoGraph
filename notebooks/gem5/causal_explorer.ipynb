{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check dependencies exist before doing all the work\n",
    "import networkx as nx\n",
    "import causalnex\n",
    "from causalnex.plots import plot_structure\n",
    "from causalnex.structure import StructureModel\n",
    "\n",
    "# Make sure to have graphviz installed and pygraphviz:\n",
    "# brew install graphviz\n",
    "# sudo apt-get install graphviz graphviz-dev\n",
    "# pip install graphviz\n",
    "# pip install pygraphviz\n",
    "plot_structure(StructureModel())\n",
    "print(causalnex.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from autorocks.data.loader.all_models_result_aggregator import create_all_models_comparison_dataset\n",
    "from autorocks.envs.gem5.schema import Gem5ParametersCollection20\n",
    "\n",
    "# Capture groups OLD\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from autorocks.dir_struct import LocalResultDir\n",
    "from autorocks.envs.gem5.benchmarks.benchmark_tasks import MachSuiteTask\n",
    "\n",
    "task_name = str(MachSuiteTask.AES)\n",
    "exp_dir = LocalResultDir / f\"gem5/{task_name}/20_params/100_iter\"\n",
    "model_comparison_data = create_all_models_comparison_dataset(exp_dir)\n",
    "\n",
    "param_space = Gem5ParametersCollection20()\n",
    "param_names = set([p.name for p in param_space.parameters()])\n",
    "main_targets = [\"bench_stats.avg_power\", \"detailed_stats.system.sim_seconds\"]\n",
    "\n",
    "# # to latex\n",
    "# import pandas as pd\n",
    "#\n",
    "# pd.DataFrame(param_space.to_latex(),\n",
    "#              columns=['name', 'lower bound', 'upper bound']).to_csv('input_params.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from notebooks.gem5.statistics_parser import all_models_parser\n",
    "\n",
    "df = all_models_parser(exp_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for col in df.performance.columns:\n",
    "    print(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combine all models data and flatten the structure\n",
    "system_pref = model_comparison_data.system_performance\n",
    "params_df = model_comparison_data.parameters\n",
    "extra_perf = df.performance\n",
    "\n",
    "model_filter = \"BoGraph\"\n",
    "system_pref = system_pref[system_pref.model == model_filter]\n",
    "params_df = params_df[model_comparison_data.parameters.model == model_filter]\n",
    "extra_perf = extra_perf[df.performance.model == model_filter]\n",
    "\n",
    "system_pref = system_pref[[\"bench_stats.avg_power\", \"detailed_stats.system.sim_seconds\", \"step\", \"iteration\"]]\n",
    "params_df = params_df.drop(columns=[\"model\"])\n",
    "extra_perf = extra_perf.drop(columns=[\"model\"]).fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove low variance features\n",
    "\n",
    "sel = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))\n",
    "sel.fit_transform(extra_perf)\n",
    "extra_perf_no_low = extra_perf.loc[:, sel.get_support()]\n",
    "# extra_perf_no_low['step'] = extra_perf['step'].copy()\n",
    "# extra_perf_no_low['iteration'] = extra_perf['iteration'].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## new grouping\n",
    "main_groups = defaultdict(list)\n",
    "for metric in extra_perf_no_low.columns:\n",
    "    if \"::\" in metric:\n",
    "        continue  # histogram, ignore\n",
    "    groups = metric.split(\".\")\n",
    "    if not groups or len(groups) < 2:\n",
    "        continue\n",
    "    main_groups[f\"{groups[1]}_{groups[-2]}\"].append(metric)\n",
    "\n",
    "for group, val in main_groups.items():\n",
    "    print(f\"{group} has: {len(val)} items\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option B\n",
    "import re\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure all within same ranges\n",
    "scaler = StandardScaler()\n",
    "sub_group_extractor = re.compile(\".*\\.([^.]+)\", re.RegexFlag.IGNORECASE)\n",
    "group_extractor = re.compile(\"([^.]+)\", re.RegexFlag.IGNORECASE)\n",
    "main_groups = defaultdict(list)\n",
    "sub_groups = defaultdict(list)\n",
    "\n",
    "# All columns minus the none static one\n",
    "idx_cols = {\"iteration\", \"step\"}\n",
    "values_cols = set(extra_perf_no_low.columns) - idx_cols\n",
    "\n",
    "for metric in values_cols:\n",
    "    groups = group_extractor.findall(metric)\n",
    "    if not groups or len(groups) < 2:\n",
    "        continue\n",
    "    sub_name = sub_group_extractor.findall(metric)\n",
    "\n",
    "    main_groups[groups[1]].append(sub_name[0])\n",
    "    sub_groups[sub_name[0]].append(metric)\n",
    "\n",
    "# import pandas as pd\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "metric_pruned = extra_perf_no_low.copy()\n",
    "\n",
    "for group, sub_group_names in main_groups.items():\n",
    "    # print(f\"{group} has: {len(sub_metrics)} items\")\n",
    "    group_vals = []\n",
    "\n",
    "    for sub_group_name in sub_group_names:\n",
    "        sub_metrics = sub_groups[sub_group_name]\n",
    "        scaler = StandardScaler()\n",
    "        scaled_vals = scaler.fit_transform(metric_pruned[sub_metrics].values)\n",
    "        if len(sub_metrics) > 1:\n",
    "            # reduce it to 1.\n",
    "            transformer = PCA(n_components=1)\n",
    "            decomposed_vals = transformer.fit_transform(scaled_vals)\n",
    "            group_vals.append(decomposed_vals.squeeze())\n",
    "        else:\n",
    "            group_vals.append(scaled_vals.squeeze())\n",
    "    group_vals_ = np.vstack(group_vals).T\n",
    "    group_transformer = PCA(n_components=1)\n",
    "    group_pruned_val = group_transformer.fit_transform(group_vals_)\n",
    "    metric_pruned[group] = group_pruned_val\n",
    "new_cols = set(list(main_groups.keys()) + [\"iteration\", \"step\"])\n",
    "metric_pruned = metric_pruned[new_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sklearn.preprocessing as pre\n",
    "\n",
    "# normalize parameters\n",
    "params_df_scaled = params_df.copy()\n",
    "scaler = StandardScaler()  # pre.MinMaxScaler()#StandardScaler()\n",
    "all_cols_no_idx = set(params_df_scaled.columns) - {\"step\", \"iteration\"}\n",
    "params_df_scaled_val = scaler.fit_transform(params_df_scaled[all_cols_no_idx].values)\n",
    "params_df_scaled.loc[:, all_cols_no_idx] = params_df_scaled_val\n",
    "\n",
    "# standardize res\n",
    "\n",
    "system_pref_scaled = system_pref.copy()\n",
    "scaler = pre.StandardScaler()  # StandardScaler()\n",
    "all_cols_no_idx = set(system_pref_scaled.columns) - {\"step\", \"iteration\"}\n",
    "system_pref_scaled_val = scaler.fit_transform(system_pref_scaled[all_cols_no_idx].values)\n",
    "system_pref_scaled.loc[:, all_cols_no_idx] = system_pref_scaled_val\n",
    "\n",
    "# Merge data\n",
    "param_targets = params_df_scaled.merge(system_pref_scaled, on=[\"step\", \"iteration\"])\n",
    "param_targets = param_targets.merge(metric_pruned, on=[\"step\", \"iteration\"])\n",
    "param_targets = param_targets.drop(\n",
    "    columns=[\n",
    "        \"iteration\",\n",
    "        \"step\",\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Structure between main objectives"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_targets = [\n",
    "    \"bench_stats.avg_power\",\n",
    "    \"detailed_stats.system.sim_seconds\",\n",
    "]\n",
    "from causalnex.structure.pytorch import from_pandas\n",
    "import torch\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "sm = from_pandas(\n",
    "    param_targets,\n",
    "    # w_threshold=0.8,\n",
    "    tabu_parent_nodes=main_targets,\n",
    "    tabu_child_nodes=param_names,\n",
    "    # hidden_layer_units=[1],\n",
    "    # ridge_beta=0.1\n",
    ")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_targets = [\"bench_stats.avg_power\", \"detailed_stats.system.sim_seconds\", \"EDP\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manual search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nodes_to_keep = set()\n",
    "\n",
    "for node in nx.shortest_path(smaller_sm, source=\"cycle_time\", target=\"EDP\"):\n",
    "    nodes_to_keep.add(node)\n",
    "for node in nx.shortest_path(smaller_sm, source=\"enable_l2\", target=\"EDP\"):\n",
    "    nodes_to_keep.add(node)\n",
    "for node in nx.shortest_path(smaller_sm, source=\"cache_line_sz\", target=\"EDP\"):\n",
    "    nodes_to_keep.add(node)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "smaller_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from autorocks.viz.causal_util import clean_node_name\n",
    "\n",
    "\n",
    "node_names = []\n",
    "for node in smaller_sm.nodes:\n",
    "    node_names.append(clean_node_name(node))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_graph = nx.DiGraph()\n",
    "for u, v in smaller_sm.edges():\n",
    "    new_graph.add_edge(clean_node_name(u), clean_node_name(v))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nx.nx_agraph.to_agraph(new_graph).layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for node in new_graph.nodes:\n",
    "    print(f'\"{node}\";')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pygraphviz as pgv\n",
    "\n",
    "nx.nx_agraph.to_agraph(new_graph).draw(f, format=\"dot\", prog=\"nop\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from autorocks.viz.causal_util import plot_struct_customized\n",
    "from IPython.display import Image\n",
    "\n",
    "smaller_sm = sm.copy()\n",
    "smaller_sm.remove_edges_below_threshold(0.35)\n",
    "# smaller_sm.remove_edge(\"detailed_stats.system.sim_seconds\", \"cpu\")\n",
    "smaller_sm.add_edges_from(\n",
    "    [\n",
    "        (\"detailed_stats.system.sim_seconds\", \"EDP\", {\"weight\": 3, \"expert\": True}),\n",
    "        (\"bench_stats.avg_power\", \"EDP\", {\"weight\": 3, \"expert\": True}),\n",
    "        (\"cpu\", \"bench_stats.avg_power\", {\"weight\": 2}),\n",
    "    ],\n",
    "    origin=\"expert\",\n",
    ")\n",
    "\n",
    "# smaller_sm.add_edges_from(smaller_in,\n",
    "#                           origin=\"learned\")\n",
    "smaller_sm = smaller_sm.get_largest_subgraph()\n",
    "\n",
    "smaller_sm.remove_nodes_from(set(smaller_sm.nodes) - nodes_to_keep)\n",
    "f = f\"{task_name}_structure.dot\"\n",
    "viz = plot_struct_customized(\n",
    "    smaller_sm, graph_name=f\"Structure for {task_name}\", param_nodes=param_names, sink_nodes=main_targets\n",
    ")\n",
    "# obj_subgraph = viz.subgraph(['bench_stats.avg_power', 'detailed_stats.system.sim_seconds'], name=\"objectives\")\n",
    "# obj_subgraph.graph_attr.update({\"rank\": \"same\"})\n",
    "viz.draw(f, format=\"dot\")\n",
    "# Image(f)\n",
    "Image(viz.draw(format=\"png\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nx.predecessor(smaller_sm, source=\"param_space\", target=\"EDP\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from autorocks.viz.causal_util import plot_struct_customized\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "out_dir = \"/home/salabed/workspace/latex/papers/osdi21_bograph/figs\"\n",
    "\n",
    "viz = plot_struct_customized(\n",
    "    smaller_sm, graph_name=f\"Structure for {task_name}\", param_nodes=param_names, sink_nodes=main_targets\n",
    ")\n",
    "\n",
    "name = f\"{task_name}_structure\"\n",
    "f = f\"{out_dir}/svg/{name}.svg\"\n",
    "viz.draw(f, format=\"svg\")\n",
    "f = f\"{out_dir}/{name}.pdf\"\n",
    "# viz.draw(f, format='pdf')\n",
    "\n",
    "Image(viz.draw(format=\"png\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max(smaller_sm.degree, key=lambda x: x[1])[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "found_nodes = []\n",
    "\n",
    "for node in sm.get_target_subgraph(main_targets[0]).nodes():\n",
    "    if node in param_names:\n",
    "        found_nodes.append(node)\n",
    "\n",
    "print(found_nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# OLD ST UFF\n",
    "\n",
    "import re\n",
    "\n",
    "group_extractor = re.compile(\"([^.]+)\", re.RegexFlag.IGNORECASE)\n",
    "main_groups = defaultdict(list)\n",
    "for metric in extra_perf_no_low.columns:\n",
    "    groups = group_extractor.findall(metric)\n",
    "    if not groups or len(groups) < 2:\n",
    "        continue\n",
    "    main_groups[groups[1]].append(metric)\n",
    "\n",
    "for group, val in main_groups.items():\n",
    "    print(f\"{group} has: {len(val)} items\")\n",
    "### Option A\n",
    "# import pandas as pd\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "metric_pruned = extra_perf_no_low.copy()\n",
    "scaler = StandardScaler()\n",
    "all_cols_no_idx = set(metric_pruned.columns) - {\"step\", \"iteration\"}\n",
    "metric_pruned.loc[:, all_cols_no_idx] = scaler.fit_transform(metric_pruned[all_cols_no_idx].values)\n",
    "\n",
    "\n",
    "for group, sub_metrics in main_groups.items():\n",
    "    # print(f\"{group} has: {len(sub_metrics)} items\")\n",
    "    group_vals = []\n",
    "    scaled_vals = metric_pruned[sub_metrics]\n",
    "    if len(sub_metrics) > 1:\n",
    "        # reduce it to 1.\n",
    "        transformer = PCA(n_components=1)\n",
    "        decomposed_vals = transformer.fit_transform(scaled_vals)\n",
    "        group_vals.append(decomposed_vals.squeeze())\n",
    "    else:\n",
    "        group_vals.append(scaled_vals.squeeze())\n",
    "    group_vals_ = np.vstack(group_vals).T\n",
    "    # group_transformer = FactorAnalysis(n_components=1)\n",
    "    # group_pruned_val = group_transformer.fit_transform(group_vals_)\n",
    "    metric_pruned[group] = group_vals_\n",
    "#\n",
    "new_cols = set(list(main_groups.keys()) + [\"iteration\", \"step\"])\n",
    "#\n",
    "metric_pruned = metric_pruned[new_cols]\n",
    "metric_pruned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metric_pruned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_groups.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### TOOO OLD\n",
    "# BACKUP Option b\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re\n",
    "\n",
    "# Ensure all within same ranges\n",
    "scaler = StandardScaler()\n",
    "sub_group_extractor = re.compile(\".*\\.([^.]+)\", re.RegexFlag.IGNORECASE)\n",
    "sub_groups = defaultdict(list)\n",
    "\n",
    "# All columns minus the none static one\n",
    "idx_cols = {\"iteration\", \"step\"}\n",
    "values_cols = set(extra_perf_no_low.columns) - idx_cols\n",
    "\n",
    "for metric in values_cols:\n",
    "    sub_name = sub_group_extractor.findall(metric)\n",
    "    if not sub_name:\n",
    "        continue\n",
    "    sub_groups[sub_name[0]].append(metric)\n",
    "\n",
    "# import pandas as pd\n",
    "metric_pruned = extra_perf_no_low.copy()\n",
    "\n",
    "for group, sub_metrics in sub_groups.items():\n",
    "    # print(f\"{group} has: {len(sub_metrics)} items\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(metric_pruned[sub_metrics].values)\n",
    "    if len(sub_metrics) > 1:\n",
    "        # reduce it to 1.\n",
    "        transformer = PCA(n_components=1)\n",
    "        decomposed_vals = transformer.fit_transform(scaled_vals)\n",
    "        metric_pruned[group] = decomposed_vals.squeeze()\n",
    "    else:\n",
    "        metric_pruned[group] = scaled_vals.squeeze()\n",
    "new_cols = set(list(sub_groups.keys()) + [\"iteration\", \"step\"])\n",
    "\n",
    "metric_pruned = metric_pruned[new_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metric_pruned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_targets = params_df.merge(system_pref, on=[\"step\", \"iteration\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = system_pref[[\"bench_stats.avg_power\", \"detailed_stats.system.sim_seconds\"]].values\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = y[:, 0] * np.log((1 / y[:, 1]) * (1 / y[:, 1]))\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X = metric_pruned.drop(columns=[\"step\", \"iteration\"])\n",
    "\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "selector = SelectKBest(f_regression, k=50)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "X_support = selector.get_support()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "small_metric = metric_pruned.drop(columns=[\"step\", \"iteration\"])\n",
    "metric_pruned = small_metric.loc[:, selector.get_support()]\n",
    "metric_pruned[\"step\"] = extra_perf_no_low[\"step\"].copy()\n",
    "metric_pruned[\"iteration\"] = extra_perf_no_low[\"iteration\"].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metric_pruned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "param_targets[\"edp\"] = np.log(\n",
    "    param_targets[\"bench_stats.avg_power\"].values * (param_targets[\"bench_stats.cycle\"].values ** 2)\n",
    ")\n",
    "\n",
    "param_targets[[\"edp\", \"cycle_time\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.lineplot(data=param_targets[[\"edp\", \"cycle_time\"]], x=\"cycle_time\", y=\"edp\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_targets[[\"edp\", \"cycle_time\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_targets[[\"edp\", \"cycle_time\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_targets[[\"edp\", \"cycle_time\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_targets[[\"edp\", \"cycle_time\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_targets[[\"edp\", \"cycle_time\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
