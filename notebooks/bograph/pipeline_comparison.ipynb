{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:52:13.312525Z",
     "start_time": "2024-01-23T21:52:13.236437Z"
    }
   },
   "source": [
    "import pathlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:52:22.051391Z",
     "start_time": "2024-01-23T21:52:13.311878Z"
    }
   },
   "id": "a64adbe729875a7a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from autorocks.data.loader.all_models_result_aggregator import create_all_models_comparison_dataset\n",
    "from autorocks.dir_struct import PackageRootDir\n",
    "from sysgym.envs.rocksdb import schema\n",
    "\n",
    "param_space = schema.RocksDB10Params()\n",
    "\n",
    "all_model_df = create_all_models_comparison_dataset(PackageRootDir / \"ProcessedDataNew/rocksdb/iops/zippy_workload_15min/10_params/50_iter\", save_results=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:52:25.054008Z",
     "start_time": "2024-01-23T21:52:22.046737Z"
    }
   },
   "id": "9d4a0cc939cf6fa1",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "combined_df = pd.merge(all_model_df.sys_params, all_model_df.sys_observations, on=[\"model\", \"iteration\", \"step\"]).drop(columns=[\"model\", \"iteration\", \"step\", \"db_bench.mixgraph.name\"])\n",
    "combined_df.rename(columns=lambda x: re.sub(r'^db_bench\\.mixgraph\\.', '', x), inplace=True)\n",
    "combined_df.rename(columns=lambda x: re.sub('rocksdb[\\_]?', '', x), inplace=True)\n",
    "combined_df.rename(columns=lambda x: re.sub(r'_statistics', '', x), inplace=True)\n",
    "combined_df.rename(columns=lambda x: re.sub(r'_stats', '', x), inplace=True)\n",
    "combined_df.rename(columns=lambda x: re.sub(r'^statistics.', '', x), inplace=True)\n",
    "combined_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:52:25.657826Z",
     "start_time": "2024-01-23T21:52:25.056483Z"
    }
   },
   "id": "13d6f65a3ee49b79",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "intermediate_col = set(combined_df.columns) - set(param_space) -{\"iops\", \"exe_time\", \"cpu_usage.count\", \"cpu_usage.p100\", \"cpu_usage.p99\",\n",
    "                                                                 \"cpu_usage.p95\", \"cpu_usage.p90\", \"cpu_usage.p50\",\n",
    "\"cpu_usage.sum\",\n",
    "                                                                 \"mem_usage.count\", \"mem_usage.p100\", \"mem_usage.p99\",\n",
    "                                                                 \"mem_usage.p95\", \"mem_usage.p90\", \"mem_usage.p50\",\n",
    "                                                                 \"mem_usage.sum\",\n",
    "                                                                 }\n",
    "intermediate_df = combined_df[list(intermediate_col)]\n",
    "# Remove string\n",
    "intermediate_df = intermediate_df[intermediate_df.T[intermediate_df.dtypes != object].index]\n",
    "intermediate_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:58:53.700378Z",
     "start_time": "2024-01-23T21:58:53.628208Z"
    }
   },
   "id": "36efd22948f9cf97",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "list(sorted(intermediate_df.columns))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:58:55.029678Z",
     "start_time": "2024-01-23T21:58:54.951977Z"
    }
   },
   "id": "9055b044063a6b51",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create test/training dataset, split to 100 train, the rest will be used to test\n",
    "\n",
    "X_FULL = combined_df[list(param_space)]\n",
    "Y_FULL = combined_df[\"iops\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:58:56.812731Z",
     "start_time": "2024-01-23T21:58:56.783450Z"
    }
   },
   "id": "5055804fb3ea9d89",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "\n",
    "\n",
    "from autorocks.optimizer.bograph import preprocessor\n",
    "from autorocks.optimizer.bograph.dag_preprocessor import PreprocessingPipeline\n",
    "from autorocks.optimizer.bograph.bograph_dao import BoGraphDataPandas\n",
    "\n",
    "data = BoGraphDataPandas(params=combined_df[list(param_space)].copy(), objs=combined_df[\"iops\"].copy(), intermediate=intermediate_df.copy())\n",
    "\n",
    "dp = PreprocessingPipeline(\n",
    "    preprocessors=[\n",
    "        # Add average for count\n",
    "        preprocessor.GrouperProcessor(-2, preprocessor.Compressor.COMBINER),\n",
    "        # If there are any with useful statistics get it.\n",
    "        preprocessor.FilterProcessor(-2 ),\n",
    "        preprocessor.VarianceThresholdPreprocessor(),\n",
    "        preprocessor.MetricsStandardizerProcessor(standardize_params=True),\n",
    "        # preprocessor.ParamNormalizerProcessor(param_space.bounds().T),\n",
    "        preprocessor.RankerProcessor(top_k = 15),\n",
    "    ]\n",
    ")\n",
    "\n",
    "processed_data = dp.fit_transform(data)\n",
    "sorted(processed_data.intermediate.columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T22:17:24.833550Z",
     "start_time": "2024-01-23T22:17:24.743704Z"
    }
   },
   "id": "45e40a51e008643e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from autorocks.optimizer.bograph import dag_discovery\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "full_G = dag_discovery.learn_dag(processed_data, dag_type=dag_discovery.DAGType.FULL)\n",
    "learning_time = time.time() - start_time\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "id": "336863a64bf79eb4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import networkx as nx\n",
    "from autorocks.optimizer.bograph import dag_postprocessor\n",
    "\n",
    "main_targets = {'iops'}\n",
    "# Prune anything not coming out of sources\n",
    "full_dag_pro = dag_postprocessor.postprocess_structure(full_G, set(param_space) ,{\"iops\"})\n",
    "max_dim = max(full_dag_pro.in_degree, key=lambda x: x[1])\n",
    "print(f\"{max_dim=}, num nodes: {len(full_dag_pro.nodes)}\")\n",
    "\n",
    "nx.draw_networkx(full_dag_pro, pos=nx.spring_layout(full_dag_pro))\n",
    "# nx.write_gpickle(full_dag_pro, \"full_dag_pro.gpickle\") # Good\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "id": "9df1a9e30b8b8662",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class ExperimentResult(NamedTuple):\n",
    "    model: str\n",
    "    restart: int\n",
    "    step: int\n",
    "    score: float\n",
    "    runtime: float\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[M: {self.model}, R:{self.restart}, S:{self.step}]: score={self.score}, runtime={self.runtime}]\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T17:53:57.827896Z",
     "start_time": "2024-01-21T17:53:57.794325Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "id": "214223762d3e48fb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from notebooks.bobn_ch.bobn import BoBn\n",
    "import networkx as nx\n",
    "\n",
    "if full_dag_pro is None:\n",
    "    print(\"Loading graph from file\")\n",
    "    nx.read_gpickle(\"full_dag_pro.gpickle\")\n",
    "\n",
    "\n",
    "bobn_model = BoBn(full_dag_pro, param_space, {\"iops\"}, conservative_mode=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "id": "a1a8e873b6a47ea5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "\n",
    "unstructured_dag = nx.DiGraph()\n",
    "for idx, p in enumerate(param_space):\n",
    "    unstructured_dag.add_edge(p, \"iops\")\n",
    "\n",
    "unstructured_bobn = BoBn(unstructured_dag, param_space, {\"iops\"}, False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9a93e482658156",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MAIN exper\n",
    "import torch\n",
    "import baselines\n",
    "import time\n",
    "from botorch.utils.transforms import normalize, standardize\n",
    "\n",
    "from gpytorch.metrics import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "train_size = 50\n",
    "num_restarts = 1\n",
    "\n",
    "bounds = torch.from_numpy(param_space.bounds()).T\n",
    "structured_results = []\n",
    "\n",
    "data_copy = data.to_combi_pandas().copy(True)\n",
    "X_FULL = data_copy[processed_data.param_intermediates_df().columns]\n",
    "Y_FULL = data_copy['iops']\n",
    "\n",
    "# X_FULL = processed_data.param_intermediates_df()\n",
    "# Y_FULL = processed_data.objs\n",
    "\n",
    "for restart, seed in enumerate([42]):\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X_FULL, Y_FULL, train_size = train_size, random_state=seed)\n",
    "    observations = pd.merge(X_train, y_train, left_index = True, right_index = True).reset_index(drop=True)\n",
    "\n",
    "    X_test =  normalize(torch.tensor(X_test[list(param_space)].values), bounds).to(**tkwargs)\n",
    "    y_test = standardize(torch.tensor(y_test.values).to(**tkwargs))\n",
    "\n",
    "    for model_name in [\"structured\"]:\n",
    "\n",
    "        for step in range(30, train_size):\n",
    "            start_time = time.time()\n",
    "            model  = baselines.run_model(bobn_model, observations = observations[:step])\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            score = float(metrics.negative_log_predictive_density(model(X_test).mvn, y_test))\n",
    "            result = ExperimentResult(model = model_name, restart = restart, step = step, score = score, runtime = elapsed_time)\n",
    "            structured_results.append(result)\n",
    "            print(f\"{result=}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "id": "aaa5eef42dd8aabf",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "265b2b7763bab1e1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "model(X_test).mvn.mean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d6c2aa2c66fbe99",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_theme(style=\"ticks\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False})\n",
    "sns.set_context(\"paper\")  # , font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rc(\"text\", usetex=False)\n",
    "plt.rc(\"xtick\", labelsize=\"small\")\n",
    "plt.rc(\"ytick\", labelsize=\"small\")\n",
    "plt.rc(\"axes\", labelsize=\"medium\")\n",
    "plt.rc(\"pdf\", use14corefonts=True)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "sns.lineplot(results_df, x=\"step\", y=\"score\", hue=\"model\", markers = \"model\", ax=ax)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:43:53.826179Z",
     "start_time": "2024-01-21T16:43:52.365926Z"
    }
   },
   "id": "19d4eb8fae2a94ff",
   "execution_count": 180,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f1835a938b8f0c37",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "# Loop that goes through the params.\n",
    "import torch\n",
    "import baselines\n",
    "import time\n",
    "\n",
    "from gpytorch.metrics import metrics\n",
    "from botorch.utils.transforms import normalize, standardize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "bounds = torch.from_numpy(param_space.bounds()).T\n",
    "\n",
    "BOTORCH_X = X_FULL[list(param_space)].copy()\n",
    "\n",
    "train_size = 50\n",
    "num_restarts = 1\n",
    "results = []\n",
    "for restart in range(num_restarts):\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(BOTORCH_X, Y_FULL, train_size = train_size)\n",
    "\n",
    "    X_train = normalize(torch.tensor(X_train.values), bounds).to(**tkwargs)\n",
    "    X_test = normalize(torch.tensor(X_test.values), bounds).to(**tkwargs)\n",
    "\n",
    "    y_train =standardize( torch.tensor(y_train.values)).to(**tkwargs)\n",
    "    y_test = standardize(torch.tensor(y_test.values).to(**tkwargs))\n",
    "    for model_name in [\"botorch\"]:\n",
    "        for step in range(30, train_size):\n",
    "            x = X_train[:step]\n",
    "            y = y_train[:step].unsqueeze(1)\n",
    "\n",
    "            start_time = time.time()\n",
    "            model  = baselines.botorch_model(x, y, tkwargs)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = float(metrics.negative_log_predictive_density(model(X_test).mvn, y_test))\n",
    "            result = ExperimentResult(model = model_name, restart = restart, step = step, score = score, runtime = elapsed_time)\n",
    "            results.append(result)\n",
    "            print(f\"{result=}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a863df30ac66fdcc",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "bounds = torch.from_numpy(param_space.bounds()).T.to(**tkwargs)\n",
    "\n",
    "train_size = 100\n",
    "num_restarts = 1\n",
    "unstructured_results = []\n",
    "for restart in range(num_restarts):\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X_FULL, Y_FULL, train_size = train_size)\n",
    "    observations = pd.merge(X_train, y_train, left_index = True, right_index = True).reset_index(drop=True)\n",
    "\n",
    "    X_test = normalize(torch.tensor(X_test[list(param_space)].values), bounds).to(**tkwargs)\n",
    "    y_test = torch.tensor(y_test.values)\n",
    "\n",
    "    for model_name in [\"bobn_unstructured\"]:\n",
    "\n",
    "        for step in range(5, train_size):\n",
    "            start_time = time.time()\n",
    "            model  = baselines.unstructured_model(observations = observations[:step])\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            score = float(metrics.negative_log_predictive_density(model(X_test).mvn, y_test))\n",
    "            result = ExperimentResult(model = model_name, restart = restart, step = step, score = score, runtime = elapsed_time)\n",
    "            unstructured_results.append(result)\n",
    "            print(f\"{result=}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ee60cebd5a5f5b",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
